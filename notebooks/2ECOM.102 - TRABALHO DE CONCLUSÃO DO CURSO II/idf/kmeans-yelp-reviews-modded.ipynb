{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3b2f9aaa-b948-413b-ad86-de97e8c5a407",
    "_uuid": "2f2947109a189857d98ab36fb2a57c191a3e036e"
   },
   "source": [
    "## Motivation for this Notebook\n",
    "\n",
    "If I was a business owner, I would want to know how my customers are generally feeling. After reading a couple of reviews, you can start to pick up on some trends but who has the time to go through all of the comments to get a full picture of what people are saying about the company? Well luckily we have the power of NLP and Machine Learning algorithms that can do this compiling and grouping for us. Here I try to get a better look into 'average' reviews for a particular business and what's being said in them by implementing kMeans clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "1796444e-83a3-4728-8d18-c44f3cca5ce1",
    "_uuid": "eaa97b33fd28f71e2b2e7bcf2dd5361c60dd1cc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/jovyan/.local/lib/python3.6/site-packages (3.5)\n",
      "Requirement already satisfied: click in /home/jovyan/.local/lib/python3.6/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from nltk) (4.50.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from nltk) (1.0.0)\n",
      "Requirement already satisfied: regex in /home/jovyan/.local/lib/python3.6/site-packages (from nltk) (2020.11.13)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: textblob in /home/jovyan/.local/lib/python3.6/site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in /home/jovyan/.local/lib/python3.6/site-packages (from textblob) (3.5)\n",
      "Requirement already satisfied: click in /home/jovyan/.local/lib/python3.6/site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from nltk>=3.1->textblob) (4.50.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from nltk>=3.1->textblob) (1.0.0)\n",
      "Requirement already satisfied: regex in /home/jovyan/.local/lib/python3.6/site-packages (from nltk>=3.1->textblob) (2020.11.13)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import cProfile, pstats, sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.cluster import KMeans\n",
    "import textblob as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "d0a44cca-fa32-4d75-8085-400c478e302a",
    "_uuid": "a512f10ca794a19db8305046deb49afd6abb35dd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def readDataset():\n",
    "    return pd.read_json('yelp_academic_dataset_review.json', lines = True)\n",
    "\n",
    "#cProfile.run('readDataset()')\n",
    "review_df = readDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "d72928dd-47c5-4024-9089-b5ece658c4b2",
    "_uuid": "a297f198e224107cccbacd909a2dcc8dd8318da8"
   },
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer('english')\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')\n",
    "\n",
    "def tokenize(text):\n",
    "    return [snowball.stem(word) for word in tokenizer.tokenize(text.lower())]\n",
    "\n",
    "def vectorize_reviews(reviews):\n",
    "    vectorizer = TfidfVectorizer(stop_words = 'english', tokenizer = tokenize, \\\n",
    "                        min_df = 0.0025, max_df = 0.05, max_features = 1000, ngram_range = (1, 3))\n",
    "    X = vectorizer.fit_transform(reviews)\n",
    "    words = vectorizer.get_feature_names()\n",
    "    return X, words\n",
    "\n",
    "def print_clusters():\n",
    "    num_words = 20\n",
    "    X, words = vectorize_reviews(review_df['text'])\n",
    "    \n",
    "    kmeans = KMeans(n_clusters = 3)\n",
    "    kmeans.fit(X)\n",
    "    \n",
    "    common_words = kmeans.cluster_centers_.argsort()[:,-1:-num_words-1:-1]\n",
    "    for num, centroid in enumerate(common_words):\n",
    "        print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))\n",
    "\n",
    "def calc_polarity(text):\n",
    "    blob = tb.TextBlob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "def calc_subjectivity(text):\n",
    "    blob = tb.TextBlob(text)\n",
    "    return blob.sentiment.subjectivity\n",
    "\n",
    "def get_pol_sub():\n",
    "    review_df['polarity'] = review_df['text'].apply(calc_polarity)\n",
    "    review_df['subjectivity'] = review_df['text'].apply(calc_subjectivity)\n",
    "    \n",
    "    print('\\nMean Polarity: ' + str(review_df['polarity'].mean())\\\n",
    "          + '\\nMean Subjectivity: ' + str(review_df['subjectivity'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a1595bbc-72e2-415b-82dd-c259cf4b11ff",
    "_uuid": "0997abcf96b5266edc2c7a3403bb274118cbe582"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "#print_clusters()\n",
    "cProfile.run('print_clusters()')\n",
    "#cProfile.run('get_pol_sub()')\n",
    "get_pol_sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
